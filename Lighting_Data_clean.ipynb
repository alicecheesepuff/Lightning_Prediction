{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5036ea-7bc9-4ee0-937d-69d0e694a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install earthaccess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9339c642-458f-4731-be99-7e139a426ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "\n",
    "auth = earthaccess.login(strategy=\"interactive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3163336f-e84f-4e2f-b0b5-03b95e083692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GLM DOWNLOAD TO GOOGLE DRIVE/Lightning - JULY 2024\n",
      "======================================================================\n",
      "\n",
      "üîê Authenticating with NASA Earthdata...\n",
      "‚úÖ Authentication successful!\n",
      "\n",
      "======================================================================\n",
      "CONFIGURATION\n",
      "======================================================================\n",
      "üìÖ Target: July 2024 (31 days)\n",
      "üó∫Ô∏è  Region: USA Continental (-130, 20, -60, 55)\n",
      "üìÅ Output: G:/My Drive/Lightning/glm_raw/2024/07\n",
      "üíæ Expected size: ~40-50 GB\n",
      "‚è±Ô∏è  Estimated time: 4-6 hours\n",
      "üìä Available space in Drive: ~94 GB\n",
      "======================================================================\n",
      "\n",
      "üöÄ Starting download to Google Drive/Lightning folder\n",
      "\n",
      "======================================================================\n",
      "\n",
      "üìÅ Files will be saved to:\n",
      "   G:/My Drive/Lightning/glm_raw/2024/07\n",
      "\n",
      "üìÖ Day 01/31: July 1, 2024\n",
      "----------------------------------------------------------------------\n",
      "üîç Searching...\n",
      "‚úÖ Found: 2000 files\n",
      "‚¨áÔ∏è  Downloading to Google Drive/Lightning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0d3a026e6f4401abe8682e2595f676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4da2a099792e42dc8bc986dcf1a41f24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "‚è∏Ô∏è  Download interrupted by user\n",
      "   All progress saved in Google Drive/Lightning\n",
      "   You can resume by running this script again.\n",
      "\n",
      "======================================================================\n",
      "DOWNLOAD COMPLETE - JULY 2024\n",
      "======================================================================\n",
      "\n",
      "üìä Files:\n",
      "   New downloads: 0\n",
      "   Already existing: 0\n",
      "   Total: 0\n",
      "\n",
      "üìà Coverage:\n",
      "   Expected: ~44,640 files\n",
      "   Actual: 0 files\n",
      "   Coverage: 0.0%\n",
      "\n",
      "‚è±Ô∏è  Time:\n",
      "   Total elapsed: 0.07 hours\n",
      "\n",
      "üìÅ Location:\n",
      "   Google Drive ‚Üí Lightning ‚Üí glm_raw ‚Üí 2024 ‚Üí 07\n",
      "   Direct path: G:/My Drive/Lightning/glm_raw/2024/07\n",
      "\n",
      "üéâ All 31 days downloaded successfully!\n",
      "\n",
      "======================================================================\n",
      "NEXT STEPS\n",
      "======================================================================\n",
      "‚úÖ Raw data saved to Google Drive/Lightning/glm_raw\n",
      "‚û°Ô∏è  Step 1: Run hourly aggregation script (next)\n",
      "‚û°Ô∏è  Step 2: Delete raw files (saves ~45 GB)\n",
      "‚û°Ô∏è  Step 3: Load hourly files to Jupyter (~1.5 GB only)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DOWNLOAD JULY 2024 GLM DATA TO GOOGLE DRIVE - Lightning Folder\n",
    "# ============================================================================\n",
    "\n",
    "# Primero instala earthaccess si no lo tienes\n",
    "# !pip install earthaccess\n",
    "\n",
    "import earthaccess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GLM DOWNLOAD TO GOOGLE DRIVE/Lightning - JULY 2024\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# AUTHENTICATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"üîê Authenticating with NASA Earthdata...\")\n",
    "auth = earthaccess.login(strategy=\"interactive\")\n",
    "print(\"‚úÖ Authentication successful!\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "COLLECTION_ID = \"C2278812167-GHRC_DAAC\"\n",
    "\n",
    "# USA Continental bounding box\n",
    "lat_min, lat_max = 20, 55\n",
    "lon_min, lon_max = -130, -60\n",
    "BBOX = (lon_min, lat_min, lon_max, lat_max)\n",
    "\n",
    "TARGET_YEAR = 2024\n",
    "TARGET_MONTH = 7\n",
    "NUM_DAYS = 31\n",
    "\n",
    "# Tu carpeta de Google Drive \"Lightning\"\n",
    "BASE_DIR = Path(\"G:/My Drive/Lightning/glm_raw\")\n",
    "\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"üìÖ Target: July {TARGET_YEAR} (31 days)\")\n",
    "print(f\"üó∫Ô∏è  Region: USA Continental {BBOX}\")\n",
    "print(f\"üìÅ Output: G:/My Drive/Lightning/glm_raw/2024/07\")\n",
    "print(f\"üíæ Expected size: ~40-50 GB\")\n",
    "print(f\"‚è±Ô∏è  Estimated time: 4-6 hours\")\n",
    "print(f\"üìä Available space in Drive: ~94 GB\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# SEARCH FUNCTION\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def search_glm_day(year, month, day, use_bbox=True):\n",
    "    \"\"\"Search for GLM L3 granules for a specific day.\"\"\"\n",
    "    start = f\"{year}-{month:02d}-{day:02d}T00:00:00Z\"\n",
    "    end = f\"{year}-{month:02d}-{day:02d}T23:59:59Z\"\n",
    "\n",
    "    search_params = {\n",
    "        \"concept_id\": COLLECTION_ID,\n",
    "        \"temporal\": (start, end),\n",
    "        \"count\": 2000\n",
    "    }\n",
    "\n",
    "    if use_bbox:\n",
    "        search_params[\"bounding_box\"] = BBOX\n",
    "\n",
    "    results = earthaccess.search_data(**search_params)\n",
    "    return results\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# DOWNLOAD TO GOOGLE DRIVE\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "print(\"üöÄ Starting download to Google Drive/Lightning folder\\n\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "out_dir = BASE_DIR / f\"{TARGET_YEAR}\" / f\"{TARGET_MONTH:02d}\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Files will be saved to:\")\n",
    "print(f\"   {out_dir}\\n\")\n",
    "\n",
    "total_files_downloaded = 0\n",
    "total_files_existing = 0\n",
    "failed_days = []\n",
    "start_time = time.time()\n",
    "\n",
    "for day in range(1, NUM_DAYS + 1):\n",
    "    day_start = time.time()\n",
    "\n",
    "    print(f\"üìÖ Day {day:02d}/{NUM_DAYS}: July {day}, {TARGET_YEAR}\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    try:\n",
    "        print(f\"üîç Searching...\")\n",
    "        results = search_glm_day(TARGET_YEAR, TARGET_MONTH, day, use_bbox=True)\n",
    "\n",
    "        if len(results) == 0:\n",
    "            print(f\"‚ö†Ô∏è  No data found\\n\")\n",
    "            failed_days.append(f\"July {day}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚úÖ Found: {len(results)} files\")\n",
    "\n",
    "        # Check existing files\n",
    "        day_of_year = datetime(TARGET_YEAR, TARGET_MONTH, day).timetuple().tm_yday\n",
    "        existing_pattern = f\"*_s{TARGET_YEAR}{day_of_year:03d}*.nc\"\n",
    "        existing_files = list(out_dir.glob(existing_pattern))\n",
    "\n",
    "        if len(existing_files) >= len(results) * 0.95:\n",
    "            print(f\"‚úì Already downloaded ({len(existing_files)} files)\")\n",
    "            total_files_existing += len(existing_files)\n",
    "            print(f\"‚è≠Ô∏è  Skipping to next day\\n\")\n",
    "            continue\n",
    "\n",
    "        # Download to Google Drive\n",
    "        print(f\"‚¨áÔ∏è  Downloading to Google Drive/Lightning...\")\n",
    "        files = earthaccess.download(results, out_dir.as_posix())\n",
    "\n",
    "        downloaded_count = len(files)\n",
    "        total_files_downloaded += downloaded_count\n",
    "\n",
    "        day_elapsed = time.time() - day_start\n",
    "        print(f\"‚úÖ Downloaded {downloaded_count} files in {day_elapsed/60:.1f} minutes\")\n",
    "\n",
    "        # Progress summary\n",
    "        total_so_far = total_files_downloaded + total_files_existing\n",
    "        expected_so_far = day * 1440\n",
    "        progress_pct = (day / NUM_DAYS) * 100\n",
    "\n",
    "        print(f\"üìä Progress: {day}/{NUM_DAYS} days ({progress_pct:.1f}%)\")\n",
    "        print(f\"   Total files so far: {total_so_far:,}\")\n",
    "        print(f\"   Coverage: {total_so_far / expected_so_far * 100:.1f}%\\n\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(f\"\\n\\n‚è∏Ô∏è  Download interrupted by user\")\n",
    "        print(f\"   All progress saved in Google Drive/Lightning\")\n",
    "        print(f\"   You can resume by running this script again.\")\n",
    "        break\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        failed_days.append(f\"July {day}\")\n",
    "        print(f\"   Continuing to next day...\\n\")\n",
    "        continue\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# FINAL SUMMARY\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "total_files = total_files_downloaded + total_files_existing\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DOWNLOAD COMPLETE - JULY 2024\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä Files:\")\n",
    "print(f\"   New downloads: {total_files_downloaded:,}\")\n",
    "print(f\"   Already existing: {total_files_existing:,}\")\n",
    "print(f\"   Total: {total_files:,}\")\n",
    "\n",
    "expected_files = NUM_DAYS * 1440\n",
    "coverage = (total_files / expected_files) * 100\n",
    "\n",
    "print(f\"\\nüìà Coverage:\")\n",
    "print(f\"   Expected: ~{expected_files:,} files\")\n",
    "print(f\"   Actual: {total_files:,} files\")\n",
    "print(f\"   Coverage: {coverage:.1f}%\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Time:\")\n",
    "print(f\"   Total elapsed: {elapsed_time/3600:.2f} hours\")\n",
    "if total_files_downloaded > 0:\n",
    "    print(f\"   Average speed: {elapsed_time/total_files_downloaded:.2f} sec/file\")\n",
    "\n",
    "# Storage info\n",
    "if total_files > 0:\n",
    "    sample_files = list(out_dir.glob(\"*.nc\"))[:10]\n",
    "    if sample_files:\n",
    "        avg_file_size_mb = sum(f.stat().st_size for f in sample_files) / len(sample_files) / (1024**2)\n",
    "        total_size_gb = (avg_file_size_mb * total_files) / 1024\n",
    "\n",
    "        print(f\"\\nüíæ Storage:\")\n",
    "        print(f\"   Average file size: {avg_file_size_mb:.2f} MB\")\n",
    "        print(f\"   Total size: {total_size_gb:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüìÅ Location:\")\n",
    "print(f\"   Google Drive ‚Üí Lightning ‚Üí glm_raw ‚Üí 2024 ‚Üí 07\")\n",
    "print(f\"   Direct path: {out_dir}\")\n",
    "\n",
    "if failed_days:\n",
    "    print(f\"\\n‚ö†Ô∏è  Failed/Missing days ({len(failed_days)}):\")\n",
    "    for day in failed_days:\n",
    "        print(f\"   - {day}\")\n",
    "else:\n",
    "    print(f\"\\nüéâ All 31 days downloaded successfully!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"NEXT STEPS\")\n",
    "print(\"=\" * 70)\n",
    "print(\"‚úÖ Raw data saved to Google Drive/Lightning/glm_raw\")\n",
    "print(\"‚û°Ô∏è  Step 1: Run hourly aggregation script (next)\")\n",
    "print(\"‚û°Ô∏è  Step 2: Delete raw files (saves ~45 GB)\")\n",
    "print(\"‚û°Ô∏è  Step 3: Load hourly files to Jupyter (~1.5 GB only)\")\n",
    "print(\"=\" * 70 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "422bed70-b940-4299-92fd-af7e1ab7327c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GLM-L3 ‚Üí ERA5 GRID - PARALLEL ON LEAP\n",
      "======================================================================\n",
      "\n",
      "CONFIGURATION\n",
      "----------------------------------------------------------------------\n",
      "üìÅ Input:  /home/jovyan/Lightning_Prediction/G:/My Drive/Lightning/glm_raw/2024/07\n",
      "üìÅ Output: /home/jovyan/Lightning_Prediction/glm_hourly/2024/07\n",
      "üó∫Ô∏è  Region: USA Continental\n",
      "   Lon: -130 to -60\n",
      "   Lat: 20 to 55\n",
      "üìä Grid: ERA5 0.25¬∞ resolution\n",
      "   281 lons √ó 141 lats = 39,621 cells\n",
      "üìÖ Period: July 2024 (744 hours)\n",
      "üöÄ Workers: 6 parallel processes on LEAP\n",
      "\n",
      "‚ùå ERROR: /home/jovyan/Lightning_Prediction/G:/My Drive/Lightning/glm_raw/2024/07 no existe!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# GLM-L3 HOURLY AGGREGATION - ERA5 GRID (0.25¬∞)\n",
    "# VERSI√ìN PARALELA OPTIMIZADA PARA LEAP\n",
    "# ============================================================================\n",
    "\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import time\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "import gc\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GLM-L3 ‚Üí ERA5 GRID - PARALLEL ON LEAP\")\n",
    "print(\"=\" * 70 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION - PATHS CORREGIDOS PARA LEAP\n",
    "# ============================================================================\n",
    "\n",
    "# Path correcto en LEAP (con \"G:\" como carpeta literal)\n",
    "RAW_DIR = Path(\"/home/jovyan/Lightning_Prediction/G:/My Drive/Lightning/glm_raw/2024/07\")\n",
    "HOURLY_DIR = Path(\"/home/jovyan/Lightning_Prediction/glm_hourly/2024/07\")\n",
    "HOURLY_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# USA Continental bounds\n",
    "LON_MIN, LON_MAX = -130, -60\n",
    "LAT_MIN, LAT_MAX = 20, 55\n",
    "\n",
    "# ERA5 standard resolution\n",
    "GRID_RES = 0.25\n",
    "\n",
    "# Create ERA5 grid\n",
    "lons_era5 = np.arange(LON_MIN, LON_MAX + GRID_RES, GRID_RES)\n",
    "lats_era5 = np.arange(LAT_MIN, LAT_MAX + GRID_RES, GRID_RES)\n",
    "\n",
    "# M√ÅS WORKERS en LEAP (tienes m√°s recursos)\n",
    "N_WORKERS = 6  # Puedes aumentar a 6-8 si quieres\n",
    "\n",
    "print(\"CONFIGURATION\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"üìÅ Input:  {RAW_DIR}\")\n",
    "print(f\"üìÅ Output: {HOURLY_DIR}\")\n",
    "print(f\"üó∫Ô∏è  Region: USA Continental\")\n",
    "print(f\"   Lon: {LON_MIN} to {LON_MAX}\")\n",
    "print(f\"   Lat: {LAT_MIN} to {LAT_MAX}\")\n",
    "print(f\"üìä Grid: ERA5 {GRID_RES}¬∞ resolution\")\n",
    "print(f\"   {len(lons_era5)} lons √ó {len(lats_era5)} lats = {len(lons_era5)*len(lats_era5):,} cells\")\n",
    "print(f\"üìÖ Period: July 2024 (744 hours)\")\n",
    "print(f\"üöÄ Workers: {N_WORKERS} parallel processes on LEAP\\n\")\n",
    "\n",
    "# Verificar que los datos existen\n",
    "if not RAW_DIR.exists():\n",
    "    print(f\"‚ùå ERROR: {RAW_DIR} no existe!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "num_files = len(list(RAW_DIR.glob(\"*.nc\")))\n",
    "print(f\"‚úÖ Verificado: {num_files:,} archivos NetCDF encontrados\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def get_hour_string(year, month, day, hour):\n",
    "    return f\"{year}{month:02d}{day:02d}_{hour:02d}00\"\n",
    "\n",
    "def get_files_for_hour(year, month, day, hour):\n",
    "    \"\"\"Get all GLM files for a specific hour\"\"\"\n",
    "    day_of_year = datetime(year, month, day).timetuple().tm_yday\n",
    "    files = []\n",
    "    for minute in range(60):\n",
    "        pattern = f\"*_s{year}{day_of_year:03d}{hour:02d}{minute:02d}*.nc\"\n",
    "        files.extend(RAW_DIR.glob(pattern))\n",
    "    return sorted(files)\n",
    "\n",
    "def glm_xy_to_lonlat(x, y, proj_info):\n",
    "    \"\"\"Convert GLM geostationary projection (x,y) to geographic (lon,lat)\"\"\"\n",
    "    sat_height = proj_info.get('perspective_point_height', 35786023.0)\n",
    "    sat_lon = proj_info.get('longitude_of_projection_origin', -75.0)\n",
    "    semi_major = proj_info.get('semi_major_axis', 6378137.0)\n",
    "    semi_minor = proj_info.get('semi_minor_axis', 6356752.31414)\n",
    "    \n",
    "    H = sat_height + semi_major\n",
    "    \n",
    "    cos_x = np.cos(x)\n",
    "    cos_y = np.cos(y)\n",
    "    sin_x = np.sin(x)\n",
    "    sin_y = np.sin(y)\n",
    "    \n",
    "    a = cos_x**2 + (semi_major/semi_minor)**2 * sin_x**2\n",
    "    b = -2 * H * cos_y * cos_x\n",
    "    c = H**2 - semi_major**2\n",
    "    \n",
    "    r_s = (-b - np.sqrt(b**2 - 4*a*c)) / (2*a)\n",
    "    \n",
    "    s_x = r_s * cos_y * cos_x\n",
    "    s_y = -r_s * sin_y\n",
    "    s_z = r_s * cos_y * sin_x\n",
    "    \n",
    "    lon = sat_lon - np.degrees(np.arctan(s_y / (H - s_x)))\n",
    "    lat = np.degrees(np.arctan((semi_major/semi_minor)**2 * s_z / np.sqrt((H - s_x)**2 + s_y**2)))\n",
    "    \n",
    "    return lon, lat\n",
    "\n",
    "def aggregate_hour_to_era5_grid(files):\n",
    "    \"\"\"Aggregate GLM data to ERA5 grid - MEMORY OPTIMIZED\"\"\"\n",
    "    if len(files) == 0:\n",
    "        return None\n",
    "    \n",
    "    # Suppress stderr\n",
    "    old_stderr = sys.stderr\n",
    "    sys.stderr = open(os.devnull, 'w')\n",
    "    \n",
    "    try:\n",
    "        # Initialize output arrays\n",
    "        flash_density = np.zeros((len(lats_era5), len(lons_era5)), dtype=np.float32)\n",
    "        total_energy = np.zeros((len(lats_era5), len(lons_era5)), dtype=np.float64)\n",
    "        \n",
    "        files_processed = 0\n",
    "        coordinate_mapping_done = False\n",
    "        \n",
    "        # Process files one at a time\n",
    "        for file in files:\n",
    "            try:\n",
    "                with xr.open_dataset(file) as ds:\n",
    "                    # Only compute coordinate mapping once\n",
    "                    if not coordinate_mapping_done:\n",
    "                        x_coords = ds['x'].values\n",
    "                        y_coords = ds['y'].values\n",
    "                        proj = ds['goes_imager_projection']\n",
    "                        \n",
    "                        # Create meshgrid\n",
    "                        x_2d, y_2d = np.meshgrid(x_coords, y_coords)\n",
    "                        \n",
    "                        # Convert to lon/lat\n",
    "                        lon_2d, lat_2d = glm_xy_to_lonlat(x_2d, y_2d, proj.attrs)\n",
    "                        \n",
    "                        # Find cells within USA bounds\n",
    "                        usa_mask = (lon_2d >= LON_MIN) & (lon_2d <= LON_MAX) & \\\n",
    "                                   (lat_2d >= LAT_MIN) & (lat_2d <= LAT_MAX)\n",
    "                        \n",
    "                        # Get indices and coordinates of USA cells\n",
    "                        y_idx, x_idx = np.where(usa_mask)\n",
    "                        lon_vals = lon_2d[usa_mask]\n",
    "                        lat_vals = lat_2d[usa_mask]\n",
    "                        \n",
    "                        # Pre-compute ERA5 grid indices for each GLM cell\n",
    "                        lon_grid_idx = ((lon_vals - LON_MIN) / GRID_RES).astype(int)\n",
    "                        lat_grid_idx = ((lat_vals - LAT_MIN) / GRID_RES).astype(int)\n",
    "                        \n",
    "                        # Clip to bounds\n",
    "                        lon_grid_idx = np.clip(lon_grid_idx, 0, len(lons_era5) - 1)\n",
    "                        lat_grid_idx = np.clip(lat_grid_idx, 0, len(lats_era5) - 1)\n",
    "                        \n",
    "                        coordinate_mapping_done = True\n",
    "                        \n",
    "                        # Free memory\n",
    "                        del x_2d, y_2d, lon_2d, lat_2d, usa_mask\n",
    "                        gc.collect()\n",
    "                    \n",
    "                    # Extract data for USA region only\n",
    "                    flash_data = ds['Flash_extent_density'].values[y_idx, x_idx]\n",
    "                    energy_data = ds['Total_Optical_energy'].values[y_idx, x_idx]\n",
    "                    \n",
    "                    # Only process non-NaN cells\n",
    "                    valid = ~np.isnan(flash_data)\n",
    "                    \n",
    "                    if np.any(valid):\n",
    "                        # Accumulate to ERA5 grid\n",
    "                        np.add.at(flash_density, (lat_grid_idx[valid], lon_grid_idx[valid]), flash_data[valid])\n",
    "                        np.add.at(total_energy, (lat_grid_idx[valid], lon_grid_idx[valid]), energy_data[valid])\n",
    "                    \n",
    "                    files_processed += 1\n",
    "                    \n",
    "                    # Free memory after each file\n",
    "                    del flash_data, energy_data, valid\n",
    "                    \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        # Clean up\n",
    "        gc.collect()\n",
    "        \n",
    "        total_flashes = int(np.sum(flash_density))\n",
    "        \n",
    "        return {\n",
    "            'flash_density': flash_density,\n",
    "            'total_energy': total_energy,\n",
    "            'total_flashes': total_flashes,\n",
    "            'files_processed': files_processed\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        sys.stderr.close()\n",
    "        sys.stderr = old_stderr\n",
    "        gc.collect()\n",
    "\n",
    "def save_hourly_netcdf(data, year, month, day, hour, output_dir):\n",
    "    \"\"\"Save hourly data in ERA5-compatible format\"\"\"\n",
    "    timestamp = datetime(year, month, day, hour)\n",
    "    \n",
    "    ds = xr.Dataset(\n",
    "        data_vars={\n",
    "            \"lightning_density\": (\n",
    "                [\"latitude\", \"longitude\"],\n",
    "                data[\"flash_density\"],\n",
    "                {\n",
    "                    \"long_name\": \"Lightning flash density\",\n",
    "                    \"units\": \"flashes\",\n",
    "                    \"description\": \"Hourly accumulated flash extent density\"\n",
    "                }\n",
    "            ),\n",
    "            \"lightning_energy\": (\n",
    "                [\"latitude\", \"longitude\"],\n",
    "                data[\"total_energy\"],\n",
    "                {\n",
    "                    \"long_name\": \"Lightning optical energy\",\n",
    "                    \"units\": \"J\",\n",
    "                    \"description\": \"Hourly accumulated total optical energy\"\n",
    "                }\n",
    "            ),\n",
    "        },\n",
    "        coords={\n",
    "            \"longitude\": ([\"longitude\"], lons_era5, {\"units\": \"degrees_east\", \"long_name\": \"Longitude\"}),\n",
    "            \"latitude\": ([\"latitude\"], lats_era5, {\"units\": \"degrees_north\", \"long_name\": \"Latitude\"}),\n",
    "            \"time\": timestamp,\n",
    "        },\n",
    "        attrs={\n",
    "            \"title\": \"GLM Lightning Data on ERA5 Grid\",\n",
    "            \"source\": \"GOES-16 GLM Level 3\",\n",
    "            \"grid_resolution\": \"0.25 degrees (ERA5 standard)\",\n",
    "            \"spatial_coverage\": f\"USA Continental ({LON_MIN}, {LAT_MIN}) to ({LON_MAX}, {LAT_MAX})\",\n",
    "            \"temporal_resolution\": \"1 hour\",\n",
    "            \"created\": datetime.now().isoformat(),\n",
    "            \"total_flashes\": data[\"total_flashes\"],\n",
    "            \"files_processed\": data[\"files_processed\"],\n",
    "            \"ML_ready\": \"Compatible with ERA5 grid for machine learning\"\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    encoding = {\n",
    "        \"lightning_density\": {\"zlib\": True, \"complevel\": 5},\n",
    "        \"lightning_energy\": {\"zlib\": True, \"complevel\": 5},\n",
    "    }\n",
    "    \n",
    "    filename = f\"GLM_ERA5grid_hourly_{get_hour_string(year, month, day, hour)}.nc\"\n",
    "    filepath = output_dir / filename\n",
    "    ds.to_netcdf(filepath, encoding=encoding)\n",
    "    ds.close()\n",
    "    \n",
    "    # Free memory\n",
    "    del ds\n",
    "    gc.collect()\n",
    "\n",
    "# ============================================================================\n",
    "# PARALLEL PROCESSING\n",
    "# ============================================================================\n",
    "\n",
    "def process_single_hour(hour_info):\n",
    "    \"\"\"Process one hour - memory safe\"\"\"\n",
    "    year, month, day, hour = hour_info\n",
    "    \n",
    "    try:\n",
    "        # Check if already exists\n",
    "        filename = f\"GLM_ERA5grid_hourly_{get_hour_string(year, month, day, hour)}.nc\"\n",
    "        out_path = HOURLY_DIR / filename\n",
    "        \n",
    "        if out_path.exists():\n",
    "            return (day, hour, \"exists\", 0, 0, 0)\n",
    "        \n",
    "        # Get files\n",
    "        files = get_files_for_hour(year, month, day, hour)\n",
    "        \n",
    "        if len(files) == 0:\n",
    "            return (day, hour, \"no_files\", 0, 0, 0)\n",
    "        \n",
    "        # Process\n",
    "        start = time.time()\n",
    "        data = aggregate_hour_to_era5_grid(files)\n",
    "        \n",
    "        if data is None:\n",
    "            return (day, hour, \"failed\", 0, 0, 0)\n",
    "        \n",
    "        # Save\n",
    "        save_hourly_netcdf(data, year, month, day, hour, HOURLY_DIR)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        total_flashes = data['total_flashes']\n",
    "        n_files = data['files_processed']\n",
    "        \n",
    "        # Clean up\n",
    "        del data, files\n",
    "        gc.collect()\n",
    "        \n",
    "        return (day, hour, \"success\", total_flashes, n_files, elapsed)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return (day, hour, \"error\", 0, 0, 0)\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN - PROCESS DAY BY DAY\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROCESSING (PARALLEL BY DAY)\")\n",
    "    print(\"=\" * 70 + \"\\n\")\n",
    "    \n",
    "    YEAR = 2024\n",
    "    MONTH = 7\n",
    "    NUM_DAYS = 31\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_flashes_all = 0\n",
    "    total_files_all = 0\n",
    "    hours_processed = 0\n",
    "    \n",
    "    # PROCESS ONE DAY AT A TIME\n",
    "    for day in range(1, NUM_DAYS + 1):\n",
    "        print(f\"üìÖ Day {day:02d}/31: July {day}, {YEAR}\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Prepare hours for this day\n",
    "        day_hours = [(YEAR, MONTH, day, hour) for hour in range(24)]\n",
    "        \n",
    "        # Process this day in parallel\n",
    "        with Pool(N_WORKERS) as pool:\n",
    "            results = pool.map(process_single_hour, day_hours)\n",
    "        \n",
    "        # Display results for this day\n",
    "        for day_num, hour, status, flashes, n_files, elapsed in results:\n",
    "            hours_processed += 1\n",
    "            progress = f\"{hours_processed}/744 ({100*hours_processed/744:.1f}%)\"\n",
    "            \n",
    "            if status == \"exists\":\n",
    "                print(f\"   Hour {hour:02d}:00 - ‚úì Exists [{progress}]\")\n",
    "            elif status == \"no_files\":\n",
    "                print(f\"   Hour {hour:02d}:00 - ‚ö† No files [{progress}]\")\n",
    "            elif status == \"failed\":\n",
    "                print(f\"   Hour {hour:02d}:00 - ‚úó Failed [{progress}]\")\n",
    "            elif status == \"error\":\n",
    "                print(f\"   Hour {hour:02d}:00 - ‚úó Error [{progress}]\")\n",
    "            else:  # success\n",
    "                total_flashes_all += flashes\n",
    "                total_files_all += n_files\n",
    "                print(f\"   Hour {hour:02d}:00 - ‚úì {flashes:,} flashes, {n_files} files ({elapsed:.1f}s) [{progress}]\")\n",
    "        \n",
    "        print()  # Blank line after each day\n",
    "        \n",
    "        # Force garbage collection after each day\n",
    "        gc.collect()\n",
    "    \n",
    "    # ========================================================================\n",
    "    # FINAL SUMMARY\n",
    "    # ========================================================================\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"AGGREGATION COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\nüìä Processing:\")\n",
    "    print(f\"   Hours processed: {hours_processed}/744\")\n",
    "    print(f\"   Total flashes: {total_flashes_all:,}\")\n",
    "    print(f\"   Raw files: {total_files_all:,}\")\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Time:\")\n",
    "    print(f\"   Total: {elapsed_time/3600:.2f} hours\")\n",
    "    if hours_processed > 0:\n",
    "        print(f\"   Average: {elapsed_time/hours_processed:.1f} sec/hour\")\n",
    "    \n",
    "    # Storage stats\n",
    "    hourly_files = list(HOURLY_DIR.glob(\"GLM_ERA5grid_*.nc\"))\n",
    "    if hourly_files:\n",
    "        total_size_mb = sum(f.stat().st_size for f in hourly_files) / (1024**2)\n",
    "        \n",
    "        print(f\"\\nüíæ Storage:\")\n",
    "        print(f\"   Files: {len(hourly_files)}\")\n",
    "        print(f\"   Total: {total_size_mb/1024:.2f} GB\")\n",
    "        print(f\"   Average: {total_size_mb/len(hourly_files):.2f} MB/file\")\n",
    "    \n",
    "    print(f\"\\nüéØ Output:\")\n",
    "    print(f\"   Location: {HOURLY_DIR}\")\n",
    "    print(f\"   Format: NetCDF on ERA5 0.25¬∞ grid\")\n",
    "    print(f\"   Grid: {len(lons_era5)} √ó {len(lats_era5)} cells\")\n",
    "    print(f\"   ‚úÖ Ready for ML with ERA5 data\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273f7a5-8087-4c65-afcf-b1ebd9a8ff15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
